{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab53e2f-8004-49c9-8c8c-7748022f48ca",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install ms-fabric-cli --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d18bd04-fc22-4651-9380-27cf950a35e1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9181ec38-733e-45da-bfed-1b7fa7f44f56",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## First Time Setup\n",
    "If it´s the first time running the script, set the parameter 'FIRST_RUN' as 'True', else set it up as 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b42300-bc68-44ca-9606-09745af13378",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "FIRST_RUN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aca1a9-4941-4398-baf1-6f2be35d9f90",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Environemnt Variables\n",
    "\n",
    "- Add the target Workspace in workspace_name. It will use the target Workspace or create it.\n",
    "- Add the Capacity name in capacity_name. It will be used only if the workspace doesn´t exist and for the capacity assignment to that workspace.\n",
    "- Add the target Eventhouse name in eventhouse_name. Can be an existing Eventhouse. If blank will use detault name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630e4e0d-7813-4e19-b026-8cce0ee4f988",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "workspace_name = \"[Dev] Microsoft Fabric Platform Monitoring\"\n",
    "capacity_name=\"\" #CAPACITY NAME\n",
    "eventhouse_name = \"Fabric Platform Monitoring\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70279da6-9aa4-49c5-9d79-0d65d79647aa",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87120376-2713-4185-aa10-6272657684b5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Specify if you want to install the Capacity Module\n",
    "INSTALL_CAPACITY_MODULE = False\n",
    "\n",
    "# Capacity ID of the capacity you want to setup. You can change it manually later in the Eventstream, \n",
    "# or add more capacities\n",
    "capacity_id = f\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620b4251-8fdf-4fcd-a140-37b61badd3dd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d16656-898f-4b80-bd4a-1051d6c32476",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Specify if you want to install the Gateway Module\n",
    "INSTALL_GATEWAY_MODULE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93e835-7fed-4990-a333-a95ea976a78a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Audit and Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a936d-19d8-4bbd-8e42-f5a971bc3adc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Specify if you want to install the Platform Activity Events and Inventory Module\n",
    "# If the inventory module is used, the activity events is recommended\n",
    "INSTALL_ACTIVITY_EVENTS_MODULE = True\n",
    "INSTALL_INVENTORY_MODULE = True\n",
    "\n",
    "### For the Platform Monitoring Module, \n",
    "### you need to setup an Azure Key Vault to store the SPN with the Admin API Access,\n",
    "### and Admin access to the gateways to list. The SPN needs to be in a Servurity Group with\n",
    "### the Tenant settings access to Admin and Regular Fabric API.\n",
    "###\n",
    "### IMPORTANT: Add the Security group also to this workspace\n",
    "\n",
    "# Key Vault URI\n",
    "key_vault_uri = f\"\"\n",
    "\n",
    "# Key Vault secret name with the tenant id\n",
    "key_vault_tenant_id = f\"\"\n",
    "\n",
    "# Key vault secret name with the App Id of the Service Principal\n",
    "key_vault_client_id = f\"\"\n",
    "\n",
    "# Key vault secret name with the secret of the Service Principal\n",
    "key_vault_client_secret = f\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f305d-3afc-4466-ad88-f246ef136719",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Source Git Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7860d419-91c1-4840-9d3d-7a516750271d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "##### DO NET CHANGE UNLESS SPECIFIED OTHERWISE ####\n",
    "repo_owner = \"ecotte\"\n",
    "repo_name = \"Fabric-Monitoring-RTI\"\n",
    "branch = \"Capacity\"\n",
    "folder_prefix = \"\"\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65073be-7bb0-44a9-9bdf-c1334d166283",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37310857-6b85-4063-a1cf-0905e91d1faf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af11a20-a1fa-46f5-a2f1-59667ccc6b2d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "from zipfile import ZipFile \n",
    "import shutil\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import yaml\n",
    "import sempy.fabric as fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d04fe-3bb9-49ce-bcd5-00c41ea09405",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Download of source & config files\n",
    "This part downloads all source and config files needed for the deployment into the ressources of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e96bdf-adaf-4501-a631-a51612653c8b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def download_folder_as_zip(repo_owner, repo_name, output_zip, branch=\"main\", folder_to_extract=\"src\",  remove_folder_prefix = \"\"):\n",
    "    # Construct the URL for the GitHub API to download the repository as a zip file\n",
    "    url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/zipball/{branch}\"\n",
    "    \n",
    "    # Make a request to the GitHub API\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Ensure the directory for the output zip file exists\n",
    "    os.makedirs(os.path.dirname(output_zip), exist_ok=True)\n",
    "    \n",
    "    # Create a zip file in memory\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as zipf:\n",
    "        with zipfile.ZipFile(output_zip, 'w') as output_zipf:\n",
    "            for file_info in zipf.infolist():\n",
    "                parts = file_info.filename.split('/')\n",
    "                if  re.sub(r'^.*?/', '/', file_info.filename).startswith(folder_to_extract): \n",
    "                    # Extract only the specified folder\n",
    "                    file_data = zipf.read(file_info.filename)  \n",
    "                    if folder_prefix != \"\":\n",
    "                        parts.remove(remove_folder_prefix)\n",
    "                    output_zipf.writestr(('/'.join(parts[1:])), file_data)\n",
    "def uncompress_zip_to_folder(zip_path, extract_to):\n",
    "    # Ensure the directory for extraction exists\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    \n",
    "    # Uncompress all files from the zip into the specified folder\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    # Delete the original zip file\n",
    "    os.remove(zip_path)\n",
    "\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/src/src.zip\", branch = branch, folder_to_extract= f\"{folder_prefix}/src\", remove_folder_prefix = f\"{folder_prefix}\")\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/config/config.zip\", branch = branch, folder_to_extract= f\"{folder_prefix}/config\" , remove_folder_prefix = f\"{folder_prefix}\")\n",
    "uncompress_zip_to_folder(zip_path = \"./builtin/config/config.zip\", extract_to= \"./builtin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b96e880-7c1b-4ed5-9ea9-eb4055d41537",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Definition of deployment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5a660-9826-4865-ac50-ddf8ce578148",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def run_fab_command( command, capture_output: bool = False, silently_continue: bool = False, raw_output: bool = False):\n",
    "    result = subprocess.run([\"fab\", \"-c\", command], capture_output=capture_output, text=True)\n",
    "    if (not(silently_continue) and (result.returncode > 0 or result.stderr)):\n",
    "       raise Exception(f\"Error running fab command. exit_code: '{result.returncode}'; stderr: '{result}'\")    \n",
    "    if (capture_output and not raw_output): \n",
    "        output = result.stdout.strip()\n",
    "        return output\n",
    "    elif (capture_output and raw_output):\n",
    "        return result\n",
    "\n",
    "def fab_get_workspace_id(name):\n",
    "    result = run_fab_command(f\"get /{name} -q id\" , capture_output = True, silently_continue= True)\n",
    "    return result\n",
    "\n",
    "def fab_workspace_exists(name):\n",
    "    id = run_fab_command(f\"get /{name} -q id\" , capture_output = True, silently_continue= True)\n",
    "    return(id)\n",
    "\n",
    "def fab_delete_workspace(name):\n",
    "    result = subprocess.run([\"fab\", \"-c\", \"ls \" + str(name)], capture_output=True, text=True)\n",
    "    for item in result.stdout.split(\"\\n\"):\n",
    "        x= subprocess.run([\"fab\", \"-c\", \" del \" + str(name)+\"/\"+str(item) + \" -f\"], capture_output=True, text=True)\n",
    "        print(x)\n",
    "\n",
    "def fab_get_id(name):\n",
    "    id = run_fab_command(f\"get /{trg_workspace_name}/{name} -q id\" , capture_output = True, silently_continue= True)\n",
    "    return(id)\n",
    "\n",
    "def fab_get_item(name):\n",
    "    item = run_fab_command(f\"get /{trg_workspace_name}/{name}\" , capture_output = True, silently_continue= True)\n",
    "    return(item)\n",
    "\n",
    "def fab_get_eventstreamConnectionString(itemId):\n",
    "    item = run_fab_command(f\"api -X get /workspaces/{trg_workspace_id}/eventstreams/{itemId}/topology\" , capture_output = True, silently_continue= True)\n",
    "    topologyEventstream=json.loads(item)\n",
    "    SourceID=topologyEventstream[\"text\"][\"sources\"][0][\"id\"]\n",
    "    connection = run_fab_command(f\"api -X get /workspaces/{trg_workspace_id}/eventstreams/{itemId}/sources/{SourceID}/connection\" , capture_output = True, silently_continue= True)\n",
    "    connection = json.loads(connection)\n",
    "    return(connection[\"text\"][\"accessKeys\"][\"primaryConnectionString\"])\n",
    "\n",
    "def fab_get_display_name(name):\n",
    "    display_name = run_fab_command(f\"get /{trg_workspace_name}/{name} -q displayName\" , capture_output = True, silently_continue= True)\n",
    "    return(display_name)\n",
    "\n",
    "def fab_get_kusto_query_uri(name):\n",
    "    connection = run_fab_command(f\"get /{trg_workspace_name}/{name} -q properties.queryServiceUri -f\", capture_output = True, silently_continue= True)\n",
    "    connection = connection.split(\"\\n\")[1]\n",
    "    return(connection)\n",
    "\n",
    "def fab_get_kusto_ingest_uri(name):\n",
    "    connection = run_fab_command(f\"get /{trg_workspace_name}/{name} -q properties.ingestionServiceUri -f\", capture_output = True, silently_continue= True)\n",
    "    connection = connection.split(\"\\n\")[1]\n",
    "    return(connection)\n",
    "\n",
    "def fab_get_folders():\n",
    "    response = run_fab_command(f\"api workspaces/{trg_workspace_id}/folders\", capture_output = True, silently_continue= True)\n",
    "    return(json.loads(response).get('text',{}).get('value',[]))\n",
    "\n",
    "def fab_get_folder(folder_name):\n",
    "    for f in fab_get_folders():\n",
    "        if f.get('displayName') == folder_name:\n",
    "            return f\n",
    "    return None\n",
    "\n",
    "def fab_assign_item_folder(name,folder):\n",
    "    folder_details = fab_get_folder(folder)\n",
    "    item_id = fab_get_id(name)\n",
    "\n",
    "    if folder_details is None:\n",
    "        payload = json.dumps({\"displayName\": folder})\n",
    "        folder_details = run_fab_command(f\"api -X post workspaces/{trg_workspace_id}/folders -i {payload}\", capture_output = True, silently_continue= True)\n",
    "        folder_details = json.loads(folder_details).get('text',{})\n",
    "\n",
    "    payload = json.dumps({\"folder\": folder_details.get('id')})\n",
    "\n",
    "    return run_fab_command(f\"api -X patch workspaces/{trg_workspace_id}/items/{item_id} -i {payload}\", capture_output = True, silently_continue= True)\n",
    "\n",
    "def fab_add_schedule(name):\n",
    "    item = run_fab_command(f\"get /{trg_workspace_name}/{name} -q schedules\" , capture_output = True, silently_continue= True)\n",
    "\n",
    "    if len(json.loads(item)) == 0:\n",
    "        schedule = get_schedule_by_name(name)\n",
    "\n",
    "        return run_fab_command(f\"job run-sch /{trg_workspace_name}/{name} -i {json.dumps(schedule)}\" , capture_output = True, silently_continue=True)\n",
    "\n",
    "    return f\"\"\"Job schedule for '{name}' already exists...\n",
    "* Job schedule {item}\"\"\" \n",
    "\n",
    "def get_id_by_name(name):\n",
    "    for it in deployment_order:\n",
    "        if it.get(\"name\") == name:\n",
    "                return it.get(\"id\")\n",
    "    return None\n",
    "\n",
    "def get_schedule_by_name(name):\n",
    "    for it in deployment_order:\n",
    "        if it.get(\"name\") == name:\n",
    "                return it.get(\"schedule\")\n",
    "    return None\n",
    "\n",
    "def copy_to_tmp(name,child=None,type=None):\n",
    "    child_path = \"\" if child is None else f\".children/{child}/\"\n",
    "    type_path = \"\" if type is None else f\"{type}/\"\n",
    "    shutil.rmtree(\"./builtin/tmp\",  ignore_errors=True)\n",
    "    path2zip = \"./builtin/src/src.zip\"\n",
    "    with  ZipFile(path2zip) as archive:\n",
    "        for file in archive.namelist():\n",
    "            if file.startswith(f'src/{type_path}{name}/{child_path}'):\n",
    "                archive.extract(file, './builtin/tmp')\n",
    "    return(f\"./builtin/tmp/src/{type_path}{name}/{child_path}\" )\n",
    "\n",
    "def get_mapping_table_new_from_type(type):\n",
    "    result = \"\"\n",
    "    filtered_data = list(filter(lambda item: item['Type'] == type, mapping_table))\n",
    "    if len(filtered_data) > 0:\n",
    "        result=filtered_data[0][\"new\"]\n",
    "    return result\n",
    "\n",
    "def get_mapping_table_new_from_old(old):\n",
    "    result = \"\"\n",
    "    filtered_data = list(filter(lambda item: item['old'] == old, mapping_table))\n",
    "    if len(filtered_data) > 0:\n",
    "        result=filtered_data[0][\"new\"]\n",
    "    return result\n",
    "\n",
    "def get_mapping_table_new_from_type_item(type,item):\n",
    "    result = \"\"\n",
    "    filtered_data = list(filter(lambda table: table[\"Type\"] == type and table[\"Item\"] == item, mapping_table))\n",
    "    if len(filtered_data) > 0:\n",
    "        result=filtered_data[0][\"new\"]\n",
    "    return result\n",
    "\n",
    "def get_mapping_table_parent_type(type,item,parent_type):\n",
    "    parent_item = get_mapping_table_new_from_type_item(type,item)\n",
    "    result = get_mapping_table_new_from_type_item(parent_type,parent_item)\n",
    "    return result\n",
    "\n",
    "def replace_ids_in_folder(folder_path, mapping_table):\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(('.py', '.json', '.pbir', '.platform', '.ipynb', '.py', '.tmdl')) and not file_name.endswith('report.json'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    for mapping in mapping_table:  \n",
    "                        content = content.replace(mapping[\"old\"], mapping[\"new\"])\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(content)\n",
    "\n",
    "def replace_kqldb_parent_eventhouse(folder_parth,parent_eventhouse):\n",
    "    property_file = f\"{folder_parth}/DatabaseProperties.json\"\n",
    "    with open(property_file, 'r', encoding='utf-8') as file:\n",
    "        content = json.load(file)\n",
    "        content[\"parentEventhouseItemId\"] = fab_get_id(parent_eventhouse)\n",
    "    with open(property_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(content,file,indent=4)\n",
    "\n",
    "def replace_eventstream_destination(folder_parth,it_destinations):\n",
    "    property_file = f\"{folder_parth}/eventstream.json\"\n",
    "    with open(property_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = json.load(file)\n",
    "        destinations = content.get(\"destinations\",[])\n",
    "        for destination in destinations:\n",
    "            if destination.get(\"type\") != \"CustomEndpoint\":\n",
    "                filtered_data = list(filter(lambda table: table[\"name\"] == destination.get(\"name\") and table[\"type\"] == destination.get(\"type\"), it_destinations))\n",
    "                if len(filtered_data) > 0:        \n",
    "                    destination[\"properties\"][\"workspaceId\"] = get_mapping_table_new_from_type_item(\"Workspace Id\",trg_workspace_name)\n",
    "                    destination[\"properties\"][\"itemId\"] = get_mapping_table_new_from_type_item(\"KQL DB ID\",filtered_data[0].get(\"itemName\"))\n",
    "                    if destination.get(\"properties\",{}).get(\"databaseName\") is not None:\n",
    "                        destination[\"properties\"][\"databaseName\"] = get_mapping_table_new_from_type_item(\"KQL DB Name\",filtered_data[0].get(\"itemName\"))\n",
    "    with open(property_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(content,file,indent=4)\n",
    "\n",
    "def replace_kqldashboard_destination(folder_parth,it_datasources):\n",
    "    property_file = f\"{folder_parth}/RealTimeDashboard.json\"\n",
    "    with open(property_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = json.load(file)\n",
    "        datasources = content.get(\"dataSources\",[])\n",
    "        for datasource in datasources:\n",
    "            filtered_data = list(filter(lambda table: table[\"name\"] == datasource.get(\"name\"), it_datasources))\n",
    "            if len(filtered_data) > 0:        \n",
    "                datasource[\"workspace\"] = get_mapping_table_new_from_type_item(\"Workspace Id\",trg_workspace_name)\n",
    "                datasource[\"database\"] = get_mapping_table_new_from_type_item(\"KQL DB ID\",filtered_data[0].get(\"itemName\"))\n",
    "                datasource[\"clusterUri\"] = get_mapping_table_parent_type(\"KQL DB Eventhouse\",filtered_data[0].get(\"itemName\"),\"Kusto Query Uri\")\n",
    "    with open(property_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(content,file,indent=4)\n",
    "\n",
    "def deploy_item(name,child=None,it=None):\n",
    "    parent = \"\"\n",
    "    cli_parameter = \"\"\n",
    "\n",
    "    # Copy and replace IDs in the item\n",
    "    tmp_path = copy_to_tmp(name,child,it.get(\"type\"))\n",
    "    \n",
    "    if child is not None:\n",
    "        parent = name\n",
    "        name = child     \n",
    "\n",
    "    if \".KQLDatabase\" in name:\n",
    "        if child is not None:\n",
    "            parent = parent if eventhouse_name == \"\" or eventhouse_name is None else f\"{eventhouse_name}.Eventhouse\"\n",
    "        if it[\"parent\"] is not None:\n",
    "            parent = it[\"parent\"] if eventhouse_name == \"\" or eventhouse_name is None else f\"{eventhouse_name}.Eventhouse\"\n",
    "        mapping_table.append({\"Type\": \"KQL DB Eventhouse\", \"Item\": name, \"old\": it[\"parent\"], \"new\": parent })  \n",
    "        replace_kqldb_parent_eventhouse(tmp_path,parent)\n",
    "    elif \".Eventhouse\" in name:\n",
    "        name = name if eventhouse_name == \"\" or eventhouse_name is None else f\"{eventhouse_name}.Eventhouse\"\n",
    "    elif \".Eventstream\" in name:\n",
    "        replace_eventstream_destination(tmp_path,it[\"destinations\"]) \n",
    "    elif \".Notebook\" in name:\n",
    "        cli_parameter = cli_parameter + \" --format .py\"\n",
    "        replace_ids_in_folder(tmp_path, mapping_table)  \n",
    "    elif \".DataPipeline\" in name:\n",
    "        replace_ids_in_folder(tmp_path, mapping_table)  \n",
    "    elif \".Report\" in name:\n",
    "        replace_ids_in_folder(tmp_path, mapping_table)  \n",
    "    elif \".SemanticModel\" in name:\n",
    "        replace_ids_in_folder(tmp_path, mapping_table)\n",
    "    elif \".KQLDashboard\" in name:\n",
    "        replace_kqldashboard_destination(tmp_path, it[\"datasources\"])\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"#############################################\")\n",
    "    print(f\"Deploying {name}\")      \n",
    "    \n",
    "    run_fab_command(f\"import  /{trg_workspace_name}/{name} -i {tmp_path} -f {cli_parameter} \", silently_continue= True)\n",
    "\n",
    "    new_id= fab_get_id(name)\n",
    "\n",
    "    if \".KQLDatabase\" in name:\n",
    "        mapping_table.append({\"Type\": \"KQL DB ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "    elif \".Eventhouse\" in name:\n",
    "        query_uri = fab_get_kusto_query_uri(name)\n",
    "        ingest_uri = fab_get_kusto_ingest_uri(name)\n",
    "        mapping_table.append({\"Type\": \"Kusto Query Uri\", \"Item\": name, \"old\": it[\"kustoQueryUri\"], \"new\": query_uri })        \n",
    "        mapping_table.append({\"Type\": \"Kusto Ingest Uri\", \"Item\": name, \"old\": it[\"kustoIngestUri\"], \"new\": ingest_uri })\n",
    "        mapping_table.append({\"Type\": \"Eventhouse ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "    elif \".Eventstream\" in name:\n",
    "        if it.get(\"ConnectionStringCustomEndpoint\") is not None:\n",
    "            mapping_table.append({\"Type\": \"Connection String Evenstream\", \"Item\": name, \"old\": it[\"ConnectionStringCustomEndpoint\"], \"new\": fab_get_eventstreamConnectionString(new_id) })\n",
    "        mapping_table.append({\"Type\": \"Eventstream ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "    elif \".Notebook\" in name:\n",
    "        mapping_table.append({\"Type\": \"Notebook ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "    elif \".DataPipeline\" in name:\n",
    "        mapping_table.append({\"Type\": \"Pipeline ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "        print(\"!DO NOT FORGET TO UPDATE TRIGGER OF THE PIPELINE!\")\n",
    "    elif \".Report\" in name:\n",
    "        mapping_table.append({\"Type\": \"Report ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "    elif \".SemanticModel\" in name:\n",
    "        mapping_table.append({\"Type\": \"Semantic Model ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })\n",
    "    elif \".KQLDashboard\" in name:\n",
    "        mapping_table.append({\"Type\": \"KQLDashboard ID\", \"Item\": name, \"old\": it[\"id\"], \"new\": new_id })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a807fcaf-3b97-4952-a720-a55f75dab7a0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## CLI Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b689de-95e4-47f1-b6cd-78c0324acb06",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Set environment parameters for Fabric CLI\n",
    "token = notebookutils.credentials.getToken('pbi')\n",
    "os.environ['FAB_TOKEN'] = token\n",
    "os.environ['FAB_TOKEN_ONELAKE'] = token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefa08bf-f156-4110-941c-2325a2a6120c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Create or get current Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1252053f-d27e-4d99-aa67-e2920f4a5b0e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "base_path = './builtin/'\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/deployment_order.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        deployment_order = json.load(file)\n",
    "\n",
    "#deploy workspace idempotent\n",
    "if \"NotFound\" in fab_workspace_exists(f\"{workspace_name}.Workspace\"):\n",
    "    run_fab_command(f\"mkdir {workspace_name}.Workspace -P capacityname={capacity_name}.Capacity\")\n",
    "    print(f\"New Workspace Create\")\n",
    "\n",
    "src_workspace_name = \"Workspace.src\"\n",
    "src_workspace_id = get_id_by_name(src_workspace_name)\n",
    "\n",
    "trg_workspace_id = fab_get_workspace_id(f\"{workspace_name}.Workspace\")\n",
    "trg_workspace_name = f\"{workspace_name}.Workspace\"\n",
    "\n",
    "print(f\"Target Workspace Id: {trg_workspace_id}\")\n",
    "print(f\"Target Workspace Name: {trg_workspace_name}\")\n",
    "\n",
    "mapping_table=[]\n",
    "\n",
    "mapping_table.append({\"Type\": \"Workspace Id\", \"Item\": trg_workspace_name, \"old\": get_id_by_name(src_workspace_name), \"new\": trg_workspace_id })\n",
    "mapping_table.append({\"Type\": \"Workspace Blank Id\", \"Item\": trg_workspace_name, \"old\": \"00000000-0000-0000-0000-000000000000\", \"new\": trg_workspace_id })\n",
    "mapping_table.append({\"Type\": \"Workspace Name\", \"Item\": trg_workspace_name, \"old\": src_workspace_name, \"new\": trg_workspace_name.replace(\".Workspace\", \"\") })\n",
    "\n",
    "display(mapping_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c66dd-47bb-4394-a8b7-3772af6dd0ef",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deployment Logic\n",
    "This part iterates through all the items, gets the respective source code, replaces all IDs dynamically and deploys the new item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eaaac6-14e5-47c8-99f2-66bcdb308e00",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "exclude = [src_workspace_name]\n",
    "\n",
    "for it in deployment_order:\n",
    "\n",
    "    new_id = None\n",
    "    \n",
    "    name = it[\"name\"]\n",
    "    type = it.get(\"type\")\n",
    "\n",
    "    if not INSTALL_CAPACITY_MODULE and type == \"Capacity\":\n",
    "        continue\n",
    "    elif not INSTALL_GATEWAY_MODULE and type == \"Gateway\":\n",
    "        continue\n",
    "    elif not INSTALL_ACTIVITY_EVENTS_MODULE and type == \"ActivityEvents\":\n",
    "        continue\n",
    "    elif not INSTALL_INVENTORY_MODULE and type == \"Inventory\":\n",
    "        continue\n",
    "\n",
    "    if name in exclude:\n",
    "        continue\n",
    "\n",
    "    if not FIRST_RUN and \".Eventstream\" in name:\n",
    "        continue\n",
    "\n",
    "    deploy_item(name,None,it)\n",
    "\n",
    "    for child in it.get(\"children\",[]):\n",
    "        child_name = child[\"name\"]\n",
    "        deploy_item(name,child_name,child)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cddc4b-a1a7-4812-8bb5-23e77b8578c6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "display(mapping_table)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
